{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "c7a18346-93e5-4308-ab1c-bfcd239ad4f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PySpark SQL = Structured APIs in Spark\n",
    "#     DataFrame API (Python objects, typed operations)\n",
    "#     SQL API (spark.sql(\"SELECT ...\"))\n",
    "# Works on tabular data: rows + columns + schema.\n",
    "# Optimized by Catalyst optimizer and Tungsten execution engine.\n",
    "# Common use cases:\n",
    "#     ETL (Extract–Transform–Load)\n",
    "#     Reporting and dashboards\n",
    "#     Data warehouse style queries\n",
    "#     Joining multiple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f4ba672c-9882-4282-9861-2d71c853c721",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName(\"demo\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "83cde0c1-72cb-4646-a0da-a23ab959dc0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "11eae1d5-7b3e-4757-8c06-c23c2ae5dc6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.types import *\n",
    "emp_data = [\n",
    "    (1, \"Ravi\",   \"Sales\",   50000.0, 10),\n",
    "    (2, \"Priya\",  \"HR\",      60000.0, 20),\n",
    "    (3, \"John\",   \"Sales\",   45000.0, 10),\n",
    "    (4, \"Meera\",  \"Finance\", 70000.0, 30),\n",
    "    (5, \"Rahul\",  None,      55000.0, None)   # dept may be null\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), False),\n",
    "    StructField(\"name\",   StringType(),  True),\n",
    "    StructField(\"dept\",   StringType(),  True),\n",
    "    StructField(\"salary\", DoubleType(),  True),\n",
    "    StructField(\"dept_id\",IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp_df = spark.createDataFrame(emp_data, schema=emp_schema)\n",
    "emp_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "13fa37fd-e9fa-405b-bd1d-72e6aaaea057",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+---------+\n",
      "|dept_id|dept_name| location|\n",
      "+-------+---------+---------+\n",
      "|     10|    Sales|  Chennai|\n",
      "|     20|       HR|Bangalore|\n",
      "|     30|  Finance|   Mumbai|\n",
      "|     40|       IT|    Delhi|\n",
      "+-------+---------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dept_data = [\n",
    "    (10, \"Sales\",   \"Chennai\"),\n",
    "    (20, \"HR\",      \"Bangalore\"),\n",
    "    (30, \"Finance\", \"Mumbai\"),\n",
    "    (40, \"IT\",      \"Delhi\")\n",
    "]\n",
    "\n",
    "dept_schema = StructType([\n",
    "    StructField(\"dept_id\", IntegerType(), False),\n",
    "    StructField(\"dept_name\", StringType(), True),\n",
    "    StructField(\"location\", StringType(), True)\n",
    "])\n",
    "\n",
    "dept_df = spark.createDataFrame(dept_data, schema=dept_schema)\n",
    "dept_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4d562e0b-2091-4783-a99c-d1e2ad596c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceTempView(\"emp\")\n",
    "dept_df.createOrReplaceTempView(\"dept\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "360049bf-85c1-437c-beaf-adc52297174c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dept', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='emp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f651c2e7-d55c-49b8-adaa-09a829dd93bb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|emp_id| name| salary|\n",
      "+------+-----+-------+\n",
      "|     1| Ravi|50000.0|\n",
      "|     2|Priya|60000.0|\n",
      "|     3| John|45000.0|\n",
      "|     4|Meera|70000.0|\n",
      "|     5|Rahul|55000.0|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.select(\"emp_id\",\"name\",\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d4ba3336-e525-4bcb-b047-1f5b4975a88f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|emp_id| name| salary|\n",
      "+------+-----+-------+\n",
      "|     1| Ravi|50000.0|\n",
      "|     2|Priya|60000.0|\n",
      "|     3| John|45000.0|\n",
      "|     4|Meera|70000.0|\n",
      "|     5|Rahul|55000.0|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select emp_id,name,salary from emp\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "1c290c76-85f3-4381-90da-611120420cd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|emp_id| name| salary|\n",
      "+------+-----+-------+\n",
      "|     1| Ravi|50000.0|\n",
      "|     2|Priya|60000.0|\n",
      "|     3| John|45000.0|\n",
      "|     4|Meera|70000.0|\n",
      "|     5|Rahul|55000.0|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from pyspark.sql.functions import col\n",
    "emp_df.filter( col(\"salary\") > 30000).select(\"emp_id\",\"name\",\"salary\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ff6407d-2d80-44ca-8d5c-d8f5929efef4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "5f4b1740-7da4-4b1f-8674-72998a387947",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+\n",
      "|emp_id| name| salary|\n",
      "+------+-----+-------+\n",
      "|     1| Ravi|50000.0|\n",
      "|     2|Priya|60000.0|\n",
      "|     4|Meera|70000.0|\n",
      "|     5|Rahul|55000.0|\n",
      "+------+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "           select emp_id,name,salary\n",
    "           from emp\n",
    "           where salary >=50000 \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "93621e78-8cc9-41d6-b1cb-5f57a8e8e8df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+\n",
      "|emp_id| name| salary|dept_id|\n",
      "+------+-----+-------+-------+\n",
      "|     1| Ravi|50000.0|     10|\n",
      "|     2|Priya|60000.0|     20|\n",
      "|     3| John|45000.0|     10|\n",
      "+------+-----+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "            select emp_id,name,salary,dept_id\n",
    "            from emp\n",
    "            where dept_id in (10,20) and salary >40000 \"\"\").show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "3b81a099-8618-437c-b312-8328b9319e29",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-------------+\n",
      "|employee_id|employee_name| salary|department_id|\n",
      "+-----------+-------------+-------+-------------+\n",
      "|          4|        Meera|70000.0|           30|\n",
      "|          2|        Priya|60000.0|           20|\n",
      "|          5|        Rahul|55000.0|         NULL|\n",
      "|          1|         Ravi|50000.0|           10|\n",
      "|          3|         John|45000.0|           10|\n",
      "+-----------+-------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select \n",
    "                emp_id employee_id ,\n",
    "                name employee_name,\n",
    "                salary ,\n",
    "                dept_id department_id\n",
    "            from emp\n",
    "            order by salary desc \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "b68bf0a3-c623-4ce4-a625-16c36cc3188f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-------------+\n",
      "|employee_id|employee_name| salary|department_id|\n",
      "+-----------+-------------+-------+-------------+\n",
      "|          5|        Rahul|55000.0|         NULL|\n",
      "|          4|        Meera|70000.0|           30|\n",
      "|          2|        Priya|60000.0|           20|\n",
      "|          1|         Ravi|50000.0|           10|\n",
      "|          3|         John|45000.0|           10|\n",
      "+-----------+-------------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select \n",
    "                emp_id employee_id ,\n",
    "                name employee_name,\n",
    "                salary ,\n",
    "                dept_id department_id\n",
    "            from emp\n",
    "            order by dept, salary desc \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "72527c5d-8b50-42eb-815b-aefd98a12caf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----------+-------------+-------+-------------+---------------+---------+\n",
      "|employee_id|employee_name| salary|department_id|department_name| location|\n",
      "+-----------+-------------+-------+-------------+---------------+---------+\n",
      "|          5|        Rahul|55000.0|         NULL|           NULL|     NULL|\n",
      "|          3|         John|45000.0|           10|          Sales|  Chennai|\n",
      "|          1|         Ravi|50000.0|           10|          Sales|  Chennai|\n",
      "|          2|        Priya|60000.0|           20|             HR|Bangalore|\n",
      "|          4|        Meera|70000.0|           30|        Finance|   Mumbai|\n",
      "+-----------+-------------+-------+-------------+---------------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\" select \n",
    "                emp_id employee_id ,\n",
    "                name employee_name,\n",
    "                salary ,\n",
    "                d.dept_id department_id,\n",
    "                dept_name department_name,\n",
    "                location\n",
    "            from emp e left join dept d\n",
    "            on e.dept_id =d.dept_id\n",
    "            order by d.dept_id \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "8c5d8f35-9023-420f-9f7d-714f3a971b95",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+------------+\n",
      "| dept|total_salary|\n",
      "+-----+------------+\n",
      "|Sales|     95000.0|\n",
      "+-----+------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from pyspark.sql.functions import *\n",
    "emp_df.groupBy(\"dept\").agg(sum(col(\"salary\")).alias(\"total_salary\")).filter(col(\"total_salary\") > 70000).show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "cb51b8f2-550d-46ee-84fd-dc4871cc92f9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+---------+-------+-------+-------+---------+\n",
      "| dept|total_sal|max_sal|min_sal|avg_sal|no_of_sal|\n",
      "+-----+---------+-------+-------+-------+---------+\n",
      "|Sales|  95000.0|50000.0|45000.0|47500.0|        2|\n",
      "+-----+---------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "             select dept,\n",
    "                     sum(salary) as total_sal,\n",
    "                     max(salary) as max_sal,\n",
    "                     min(salary) as min_sal,\n",
    "                     avg(salary) as avg_sal,\n",
    "                     count(salary) as no_of_sal\n",
    "            from emp \n",
    "               group by dept \n",
    "               having sum(salary) > 70000\n",
    "               \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "15c8aee8-1f33-444d-8545-b4de523bbb9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------+---------+-------+-------+-------+---------+\n",
      "|   dept|total_sal|max_sal|min_sal|avg_sal|no_of_sal|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "|  Sales|  95000.0|50000.0|45000.0|47500.0|        2|\n",
      "|     HR|  60000.0|60000.0|60000.0|60000.0|        1|\n",
      "|Finance|  70000.0|70000.0|70000.0|70000.0|        1|\n",
      "|   NULL|  55000.0|55000.0|55000.0|55000.0|        1|\n",
      "+-------+---------+-------+-------+-------+---------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "             select dept,\n",
    "                     sum(salary) as total_sal,\n",
    "                     max(salary) as max_sal,\n",
    "                     min(salary) as min_sal,\n",
    "                     avg(salary) as avg_sal,\n",
    "                     count(salary) as no_of_sal\n",
    "            from emp \n",
    "               group by dept \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "683e7b92-d173-4ee6-8ffe-24fbf35ca7d3",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "39da81c2-26fb-4eba-90ec-ce449163a01d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----------+-----------+-------------+---------+-------+---------------+\n",
      "| name|ename_upper|ename_lower|ename_initcap|ename_sub|id_name|    id_name_sal|\n",
      "+-----+-----------+-----------+-------------+---------+-------+---------------+\n",
      "| Ravi|       RAVI|       ravi|         Ravi|      Rav| 1|Ravi| 1 Ravi 50000.0|\n",
      "|Priya|      PRIYA|      priya|        Priya|      Pri|2|Priya|2 Priya 60000.0|\n",
      "| John|       JOHN|       john|         John|      Joh| 3|John| 3 John 45000.0|\n",
      "|Meera|      MEERA|      meera|        Meera|      Mee|4|Meera|4 Meera 70000.0|\n",
      "|Rahul|      RAHUL|      rahul|        Rahul|      Rah|5|Rahul|5 Rahul 55000.0|\n",
      "+-----+-----------+-----------+-------------+---------+-------+---------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "           select name,\n",
    "                  upper(name) ename_upper,\n",
    "                  lower(name) ename_lower,\n",
    "                  initcap(name) ename_initcap,\n",
    "                  substring(name,1,3) ename_sub,\n",
    "                  concat(emp_id,'|',name) id_name,\n",
    "                  concat_ws(' ',emp_id,name,salary) id_name_sal\n",
    "            from  emp \"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ce839fbe-3507-41b2-bbcc-5d9444ddee11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---+\n",
      "| id|\n",
      "+---+\n",
      "|  0|\n",
      "+---+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = spark.range(1)\n",
    "df.createOrReplaceTempView('df_v')\n",
    "\n",
    "spark.sql(\"select * from df_v\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "bf13357c-892d-42ea-9a62-b5fd0c53ca07",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+--------------------------+\n",
      "|date      |time_stamp                |\n",
      "+----------+--------------------------+\n",
      "|2025-12-05|2025-12-05 10:47:51.880017|\n",
      "+----------+--------------------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select current_date() date, \n",
    "           current_timestamp() time_stamp\n",
    "           \"\"\").show( truncate=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "714e52ae-9e70-406d-8cb0-ad7e3d1d45d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+----+-------+-----+\n",
      "|      date|year|quarter|month|\n",
      "+----------+----+-------+-----+\n",
      "|2025-12-05|2025|      4|   12|\n",
      "+----------+----+-------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"select current_date() date,\n",
    "                  year(current_date()) year,\n",
    "                   quarter(current_date()) quarter,\n",
    "                    month (current_date()) month  \n",
    "                    \n",
    "                    \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "404c50ac-e443-4ed5-96b4-0c622542d576",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------+\n",
      "|formatted_date|\n",
      "+--------------+\n",
      "|    05-12-2025|\n",
      "+--------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "    SELECT date_format(current_date(), 'dd-MM-yyyy') AS formatted_date\n",
    "\"\"\").show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "0f232c51-f77c-443b-bd39-61ee0506800b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f21114ff-73af-4d08-a20e-bb30b66befbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+--------+--------------+----------+-------+--------+---------+--------+--------+-----------+\n",
      "|emp_id| name| salary|emp_rank|emp_dense_rank|row_number|lag_sal|lead_sal|first_sal|last_sal| sum_sal|running_sum|\n",
      "+------+-----+-------+--------+--------------+----------+-------+--------+---------+--------+--------+-----------+\n",
      "|     4|Meera|70000.0|       1|             1|         1|   NULL| 60000.0|  70000.0| 45000.0|280000.0|    70000.0|\n",
      "|     2|Priya|60000.0|       2|             2|         2|70000.0| 55000.0|  70000.0| 45000.0|280000.0|   130000.0|\n",
      "|     5|Rahul|55000.0|       3|             3|         3|60000.0| 50000.0|  70000.0| 45000.0|280000.0|   185000.0|\n",
      "|     1| Ravi|50000.0|       4|             4|         4|55000.0| 45000.0|  70000.0| 45000.0|280000.0|   235000.0|\n",
      "|     3| John|45000.0|       5|             5|         5|50000.0|    NULL|  70000.0| 45000.0|280000.0|   280000.0|\n",
      "+------+-----+-------+--------+--------------+----------+-------+--------+---------+--------+--------+-----------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select emp_id,name,salary, \n",
    "          rank() over(order by salary desc) as emp_rank,\n",
    "          dense_rank() over(order by salary desc) as emp_dense_rank,\n",
    "          row_number() over(order by salary desc) as row_number,\n",
    "          lag(salary) over(order by salary desc) as lag_sal,\n",
    "          lead(salary) over(order by salary desc) as lead_sal,\n",
    "          first_value(salary) over(order by salary desc) as first_sal,\n",
    "          last_value(salary) over(order by salary desc ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as last_sal,\n",
    "          sum(salary) over(order by salary desc ROWS BETWEEN UNBOUNDED PRECEDING AND UNBOUNDED FOLLOWING) as sum_sal,\n",
    "          sum(salary) over(order by salary desc ROWS BETWEEN UNBOUNDED PRECEDING AND CURRENT ROW) as running_sum\n",
    "          \n",
    "          from emp\n",
    "          \"\"\").show()\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "aec67451-5e22-46a7-970a-f07b70c896ac",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------------+\n",
      "|emp_id| name| salary|   dept|rank_sal_dept|\n",
      "+------+-----+-------+-------+-------------+\n",
      "|     5|Rahul|55000.0|   NULL|            1|\n",
      "|     4|Meera|70000.0|Finance|            1|\n",
      "|     2|Priya|60000.0|     HR|            1|\n",
      "|     1| Ravi|50000.0|  Sales|            1|\n",
      "|     3| John|45000.0|  Sales|            2|\n",
      "+------+-----+-------+-------+-------------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "      select emp_id,name,salary,dept, rank() over(partition by dept order by salary desc ) rank_sal_dept\n",
    "     from  emp\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "4ba89bf0-8389-4ec3-9a20-c25b2c44098d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+----+-------+--------+\n",
      "|emp_id|name| salary|emp_rank|\n",
      "+------+----+-------+--------+\n",
      "|     3|John|45000.0|       1|\n",
      "|     1|Ravi|50000.0|       2|\n",
      "+------+----+-------+--------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "         with emp_sal as(\n",
    "          select emp_id,name,salary, \n",
    "          rank() over(order by salary asc) as emp_rank from emp)\n",
    "          select * from emp_sal where emp_rank <3\n",
    "          \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "b4561f43-a3e6-4b29-8230-e3c9688934b9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     1| Ravi|  Sales|60000.0|     10|\n",
      "|     1| Ravi|  Sales|70000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     2|Priya|     HR|70000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "|     5|Rahul|   NULL|65000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "emp_data = [\n",
    "    (1, \"Ravi\",   \"Sales\",   50000.0, 10),\n",
    "    (1, \"Ravi\",   \"Sales\",   60000.0, 10),\n",
    "    (1, \"Ravi\",   \"Sales\",   70000.0, 10),\n",
    "    (2, \"Priya\",  \"HR\",      60000.0, 20),\n",
    "    (2, \"Priya\",  \"HR\",      70000.0, 20),\n",
    "    (3, \"John\",   \"Sales\",   45000.0, 10),\n",
    "    (4, \"Meera\",  \"Finance\", 70000.0, 30),\n",
    "    (5, \"Rahul\",  None,      55000.0, None),\n",
    "    (5, \"Rahul\",  None,      65000.0, None)\n",
    "]\n",
    "\n",
    "emp_schema = StructType([\n",
    "    StructField(\"emp_id\", IntegerType(), False),\n",
    "    StructField(\"name\",   StringType(),  True),\n",
    "    StructField(\"dept\",   StringType(),  True),\n",
    "    StructField(\"salary\", DoubleType(),  True),\n",
    "    StructField(\"dept_id\",IntegerType(), True)\n",
    "])\n",
    "\n",
    "emp_df1 = spark.createDataFrame(emp_data, schema=emp_schema)\n",
    "emp_df1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "b73adbf2-805b-47cf-938a-811f0fa248a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df1.createOrReplaceTempView(\"emp1\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "64865e4e-c688-434b-b1b0-3f45426ffd2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     1| Ravi|  Sales|60000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     2|Priya|     HR|70000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     1| Ravi|  Sales|70000.0|     10|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "|     5|Rahul|   NULL|65000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          select distinct * from emp1\"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "7f2d32db-c120-44b5-8011-0a72fc448c4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|70000.0|     10|\n",
      "|     2|Priya|     HR|70000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|65000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"\"\"\n",
    "          with emp_rank_data as (\n",
    "           select emp_id,name,dept,salary,dept_id , rank() over( partition by emp_id order by salary desc) as emp_rank from emp1)\n",
    "           select emp_id,name,dept,salary,dept_id from emp_rank_data where emp_rank =1\n",
    "           \"\"\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "625ad6d5-896f-4a04-8124-166e4b4051b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "emp_df.createOrReplaceGlobalTempView(\"emp_g\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "bf6fbd7f-f840-4d9d-8b04-522d3f2ef22b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.emp_g\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "91769d9d-6599-4f8d-9773-bbfddda20eb4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Table(name='dept', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='df_v', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='emp', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True),\n",
       " Table(name='emp1', catalog=None, namespace=[], description=None, tableType='TEMPORARY', isTemporary=True)]"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.listTables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "4bbf70d7-4f55-4836-ae28-87fbb5579c3e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_catalog'"
      ]
     },
     "execution_count": 69,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentCatalog()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "8ab4c9c0-e9e3-4db8-9ee2-952a48d08d38",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'default'"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.catalog.currentDatabase()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "fd8733a2-ba21-4a18-b563-15379d8ee728",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-------+-------+-------+\n",
      "|emp_id| name|   dept| salary|dept_id|\n",
      "+------+-----+-------+-------+-------+\n",
      "|     1| Ravi|  Sales|50000.0|     10|\n",
      "|     2|Priya|     HR|60000.0|     20|\n",
      "|     3| John|  Sales|45000.0|     10|\n",
      "|     4|Meera|Finance|70000.0|     30|\n",
      "|     5|Rahul|   NULL|55000.0|   NULL|\n",
      "+------+-----+-------+-------+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "spark.sql(\"select * from global_temp.emp_g\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "beecf3cf-fd03-47f4-9003-ed29556631e7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
